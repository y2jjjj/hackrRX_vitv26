{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "RIThx_3AFjmC",
        "outputId": "8afb5155-8b38-43a8-c584-6a1fb355f810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pinecone-client in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (6.0.0)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.0.0)\n",
            "Requirement already satisfied: PyPDF2 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.0.1)\n",
            "Requirement already satisfied: python-docx in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.2.0)\n",
            "Requirement already satisfied: openpyxl in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (3.1.5)\n",
            "Requirement already satisfied: python-pptx in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.0.2)\n",
            "Requirement already satisfied: pillow in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (11.3.0)\n",
            "Requirement already satisfied: pytesseract in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.13)\n",
            "Requirement already satisfied: pinecone in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (7.3.0)\n",
            "Requirement already satisfied: faiss-cpu in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.11.0.post1)\n",
            "Requirement already satisfied: certifi>=2019.11.17 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pinecone-client) (2025.7.14)\n",
            "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pinecone-client) (0.0.7)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\users\\hriday\\appdata\\roaming\\python\\python313\\site-packages (from pinecone-client) (2.9.0.post0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pinecone-client) (4.14.1)\n",
            "Requirement already satisfied: urllib3>=1.26.5 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pinecone-client) (2.5.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.54.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (0.34.3)\n",
            "Requirement already satisfied: filelock in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
            "Requirement already satisfied: requests in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-docx) (6.0.0)\n",
            "Requirement already satisfied: et-xmlfile in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from python-pptx) (3.2.5)\n",
            "Requirement already satisfied: pinecone-plugin-assistant<2.0.0,>=1.6.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pinecone) (1.7.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hriday\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.17.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\hriday\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages (run this cell first in Google Colab)\n",
        "!pip install pinecone-client sentence-transformers PyPDF2 python-docx openpyxl python-pptx pillow pytesseract pinecone faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RTlASSh3r4rv",
        "outputId": "3901fed9-5c4a-48ae-cdd0-f78a61b8f12f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: faiss-cpu in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.11.0.post1)\n",
            "Requirement already satisfied: astrapy in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.0.1)\n",
            "Requirement already satisfied: pandas in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.3.1)\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (5.0.0)\n",
            "Requirement already satisfied: requests in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (2.32.4)\n",
            "Requirement already satisfied: langchain_groq in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.3.6)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from faiss-cpu) (2.3.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: deprecation<2.2.0,>=2.1.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from astrapy) (2.1.0)\n",
            "Requirement already satisfied: httpx<1,>=0.25.2 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx[http2]<1,>=0.25.2->astrapy) (0.28.1)\n",
            "Requirement already satisfied: pymongo>=3 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from astrapy) (4.13.2)\n",
            "Requirement already satisfied: toml<0.11.0,>=0.10.2 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from astrapy) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions>=4.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from astrapy) (4.14.1)\n",
            "Requirement already satisfied: uuid6>=2024.1.12 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from astrapy) (2025.0.1)\n",
            "Requirement already satisfied: anyio in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (4.9.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (1.0.9)\n",
            "Requirement already satisfied: idna in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.25.2->httpx[http2]<1,>=0.25.2->astrapy) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx[http2]<1,>=0.25.2->astrapy) (4.2.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from h2<5,>=3->httpx[http2]<1,>=0.25.2->astrapy) (4.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hriday\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.54.1)\n",
            "Requirement already satisfied: tqdm in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (2.7.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.18.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.68 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_groq) (0.3.72)\n",
            "Requirement already satisfied: groq<1,>=0.29.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain_groq) (0.30.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (1.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from groq<1,>=0.29.0->langchain_groq) (1.3.1)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (0.4.8)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langchain-core<1.0.0,>=0.3.68->langchain_groq) (1.33)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.68->langchain_groq) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain_groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain_groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.29.0->langchain_groq) (0.4.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (3.11.1)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.68->langchain_groq) (0.23.0)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pymongo>=3->astrapy) (2.7.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hriday\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\hriday\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\hriday\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install faiss-cpu astrapy pandas sentence-transformers requests langchain_groq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QsuQtX5nFgba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Hriday\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import hashlib\n",
        "import mimetypes\n",
        "from typing import List, Dict, Any, Optional\n",
        "import logging\n",
        "\n",
        "# Core libraries\n",
        "import pinecone\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "# File processing libraries\n",
        "import PyPDF2\n",
        "from docx import Document\n",
        "import openpyxl\n",
        "from pptx import Presentation\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "import json\n",
        "import csv\n",
        "\n",
        "# Setup logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "JHOY6SliEEkC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:faiss.loader:Loading faiss with AVX2 support.\n",
            "INFO:faiss.loader:Successfully loaded faiss with AVX2 support.\n",
            "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "class FAISSRetriever:\n",
        "    def __init__(self, text: str, embedding_model: str = 'all-MiniLM-L6-v2', chunk_size: int = 500, chunk_overlap: int = 50):\n",
        "        \"\"\"\n",
        "        Initialize FAISS retriever with extracted text\n",
        "\n",
        "        Args:\n",
        "            text: The extracted text from PDF (your 'result' variable)\n",
        "            embedding_model: Sentence transformer model name\n",
        "            chunk_size: Size of text chunks\n",
        "            chunk_overlap: Overlap between chunks\n",
        "        \"\"\"\n",
        "        self.text = text\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "        # Initialize embedding model\n",
        "        self.embedding_model = SentenceTransformer(embedding_model)\n",
        "\n",
        "        # Process text and create index\n",
        "        self.chunks = self._chunk_text(text)\n",
        "        self.embeddings = self._create_embeddings(self.chunks)\n",
        "        self.index = self._create_faiss_index(self.embeddings)\n",
        "\n",
        "        print(f\"✅ Created FAISS index with {len(self.chunks)} chunks\")\n",
        "\n",
        "    def _chunk_text(self, text: str) -> List[str]:\n",
        "        \"\"\"Split text into overlapping chunks\"\"\"\n",
        "        if len(text) <= self.chunk_size:\n",
        "            return [text]\n",
        "\n",
        "        chunks = []\n",
        "        start = 0\n",
        "\n",
        "        while start < len(text):\n",
        "            end = start + self.chunk_size\n",
        "            chunk = text[start:end]\n",
        "\n",
        "            # Try to end at sentence boundary\n",
        "            if end < len(text):\n",
        "                last_period = chunk.rfind('.')\n",
        "                last_newline = chunk.rfind('\\n')\n",
        "                boundary = max(last_period, last_newline)\n",
        "\n",
        "                if boundary > start + self.chunk_size // 2:\n",
        "                    chunk = text[start:start + boundary + 1]\n",
        "                    end = start + boundary + 1\n",
        "\n",
        "            chunks.append(chunk.strip())\n",
        "            start = end - self.chunk_overlap\n",
        "\n",
        "            if start >= len(text):\n",
        "                break\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def _create_embeddings(self, chunks: List[str]) -> np.ndarray:\n",
        "        \"\"\"Create embeddings for all chunks\"\"\"\n",
        "        embeddings = self.embedding_model.encode(chunks, convert_to_numpy=True)\n",
        "        return embeddings\n",
        "\n",
        "    def _create_faiss_index(self, embeddings: np.ndarray) -> faiss.Index:\n",
        "        \"\"\"Create FAISS index from embeddings\"\"\"\n",
        "        dimension = embeddings.shape[1]\n",
        "\n",
        "        # Use IndexFlatIP for cosine similarity (after normalization)\n",
        "        index = faiss.IndexFlatIP(dimension)\n",
        "\n",
        "        # Normalize embeddings for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "\n",
        "        # Add embeddings to index\n",
        "        index.add(embeddings)\n",
        "\n",
        "        return index\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Retrieve most relevant chunks for a query\n",
        "\n",
        "        Args:\n",
        "            query: User query string\n",
        "            top_k: Number of top chunks to retrieve\n",
        "\n",
        "        Returns:\n",
        "            List of tuples (chunk_text, similarity_score)\n",
        "        \"\"\"\n",
        "        # Create query embedding\n",
        "        query_embedding = self.embedding_model.encode([query], convert_to_numpy=True)\n",
        "\n",
        "        # Normalize query embedding\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search index\n",
        "        scores, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        # Return results\n",
        "        results = []\n",
        "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "            if idx < len(self.chunks):  # Valid index\n",
        "                results.append((self.chunks[idx], float(score)))\n",
        "\n",
        "        return results\n",
        "\n",
        "    def retrieve_with_context(self, query: str, top_k: int = 3) -> List[dict]:\n",
        "        \"\"\"\n",
        "        Retrieve chunks with additional context information\n",
        "\n",
        "        Returns:\n",
        "            List of dictionaries with chunk info\n",
        "        \"\"\"\n",
        "        results = self.retrieve(query, top_k)\n",
        "\n",
        "        detailed_results = []\n",
        "        for i, (chunk, score) in enumerate(results):\n",
        "            detailed_results.append({\n",
        "                'rank': i + 1,\n",
        "                'chunk': chunk,\n",
        "                'similarity_score': score,\n",
        "                'chunk_length': len(chunk),\n",
        "                'preview': chunk[:100] + \"...\" if len(chunk) > 100 else chunk\n",
        "            })\n",
        "\n",
        "        return detailed_results\n",
        "\n",
        "# Usage example:\n",
        "def create_retriever_from_result(result: str) -> FAISSRetriever:\n",
        "    \"\"\"\n",
        "    Create FAISS retriever from your extracted PDF text\n",
        "\n",
        "    Args:\n",
        "        result: Your extracted text variable\n",
        "\n",
        "    Returns:\n",
        "        FAISSRetriever instance\n",
        "    \"\"\"\n",
        "    retriever = FAISSRetriever(\n",
        "        text=result,\n",
        "        chunk_size=500,  # Adjust as needed\n",
        "        chunk_overlap=50  # Adjust as needed\n",
        "    )\n",
        "    return retriever\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "YADCdiJmEG8K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import faiss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P10qJ1tStJSy",
        "outputId": "d7aa5547-a2f5-4132-9725-85d81853aff5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
            "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PDF content extracted successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches:   0%|          | 0/8 [00:00<?, ?it/s]c:\\Users\\Hriday\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "Batches: 100%|██████████| 8/8 [00:07<00:00,  1.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created FAISS index with 252 chunks\n",
            "FAISS retriever created successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# file_path = \"/content/BAJHLIP23020V012223.pdf\"\n",
        "import requests\n",
        "import io\n",
        "\n",
        "def extract_pdf_from_url(url: str) -> str:\n",
        "   \"\"\"Extract text from PDF URL\"\"\"\n",
        "   try:\n",
        "       # Download PDF from URL\n",
        "       response = requests.get(url)\n",
        "       response.raise_for_status()\n",
        "\n",
        "       # Create a file-like object from the response content\n",
        "       pdf_file = io.BytesIO(response.content)\n",
        "\n",
        "       # Extract text using PyPDF2\n",
        "       pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "       text = \"\"\n",
        "       for page in pdf_reader.pages:\n",
        "           text += page.extract_text() + \"\\n\"\n",
        "       return text\n",
        "   except Exception as e:\n",
        "       print(f\"Error extracting PDF from URL: {str(e)}\")\n",
        "       return \"\"\n",
        "\n",
        "# Usage:\n",
        "pdf_url = \"https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D\"\n",
        "\n",
        "# result = extract_pdf_from_url(pdf_url)\n",
        "# retriever = create_retriever_from_result(result)\n",
        "def extract_pdf_content(file_path: str) -> str:\n",
        "        \"\"\"Extract text from PDF files\"\"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as file:\n",
        "                pdf_reader = PyPDF2.PdfReader(file)\n",
        "                text = \"\"\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() + \"\\n\"\n",
        "                return text\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error extracting PDF content: {str(e)}\")\n",
        "            return \"\"\n",
        "\n",
        "\n",
        "# Process the file\n",
        "#result = extract_pdf_content(file_path)\n",
        "result = extract_pdf_from_url(pdf_url)\n",
        "print(\"PDF content extracted successfully.\")\n",
        "retriever = create_retriever_from_result(result)\n",
        "print(\"FAISS retriever created successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t10X6K_qEHA9",
        "outputId": "ef8616df-dda2-4bc1-c465-c8a84f43a4c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Batches: 100%|██████████| 1/1 [00:00<00:00, 33.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chunk 1 (Score: 0.443):\n",
            ", for up to 3 months of age.  On attaining  3 months of age, the  New Born Baby  shall be covered only if \n",
            "specifically included in the Policy  mid-term and requisite premium paid to the Company.  \n",
            " \n",
            "Cover  \n",
            "Maternity Expenses  means;  \n",
            "a) Medical treatment expenses traceable to childbirth (including complicated deliveries and caesarean sections incurred during \n",
            "Hospitalization);  \n",
            "b) Expenses towards lawful medica l termination of pregnancy during the Policy Period.  \n",
            "Note:  Ectopic pregnancy i\n",
            "--------------------------------------------------\n",
            "Chunk 2 (Score: 0.415):\n",
            "within a  Waiting Period of twenty -four (24) months.  However, the Waiting Period may be waived \n",
            "only in the case of delivery, miscarriage or abortion induced by accident.   \n",
            "3. Delivery or lawful medical termination of pregnancy limited to  two deliveries or terminations or either  has been paid under \n",
            "the Policy an d its Renewals.  \n",
            "4. More than one delivery or lawful medical termination of pregnancy during a single Policy Period.  \n",
            "5. Maternity Expenses of Surrogate Mother, unless claim is\n",
            "--------------------------------------------------\n",
            "Chunk 3 (Score: 0.401):\n",
            "ng the Policy Period.  \n",
            "Note:  Ectopic pregnancy is covered under ‘In -patient treatment’, provided such pregnancy is established by medical reports.  \n",
            "  \n",
            "Exclusions  \n",
            "The Company shall not be liable to make any payment under the cover in respect of  Maternity Expenses  incurred in connection with \n",
            "or in respect of:  \n",
            "1. Covered female Insured Person  below eighteen (18) years  and above forty -five (45) years of age.  \n",
            "2. Delivery or termination within a  Waiting Period of twenty -four (24) mon\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Query for relevant chunks\n",
        "user_query = \"will I get the cover if my wife died while delivery of the child\"\n",
        "relevant_chunks = retriever.retrieve(user_query, top_k=3)\n",
        "\n",
        "# Step 3: Use the results\n",
        "for i, (chunk, score) in enumerate(relevant_chunks, 1):\n",
        "    print(f\"Chunk {i} (Score: {score:.3f}):\")\n",
        "    print(chunk)\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zdrj2YX3EHDm"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjson\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgroq\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Groq\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m userdata\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "#llama-3.1-8b-instant\n",
        "# For reasoning models\n",
        "# llama-4-maverick-17b-128e-instruct\n",
        "\n",
        "# reasoning model - setting the max reasoning tokens and all for efficiency\n",
        "\n",
        "import json\n",
        "from groq import Groq\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZKZPKrfEHFx",
        "outputId": "34a33748-0bde-454b-a072-949e284f80a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created FAISS index with 252 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "  \"answers\": {\n",
            "    \"will I get the cover if my wife died while delivery of the child\": \"No, coverage for maternity expenses is only available if the policyholder is alive.\",\n",
            "    \"what is the premium amount\": \"However, the context provided does not mention a specific premium amount. It only mentions various discounts and co-payment options. To provide a concise answer, I would need more information about the premium amount before the discounts or co-payments are applied.\\n\\nIf you provide the original premium amount, I can give you a concise answer in the format you requested. For example:\\n\\nQ: What is the premium amount after a 10% discount?\\nA: 90% of original premium amount.\\n\\nPlease provide the original premium amount for a more accurate answer.\",\n",
            "    \"what are the exclusions in this policy\": \"Critical illnesses and/or their symptoms present before policy inception or manifesting within 90 days from inception, and any claims made during a break in coverage.\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "import json\n",
        "\n",
        "def process_pdf_queries(pdf_url: str, queries_json: str, groq_api_key: str) -> str:\n",
        "    \"\"\"\n",
        "    Process PDF from URL and answer queries one by one using vector search + ChatGroq\n",
        "    Returns concise, point-to-point answers\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Initialize ChatGroq with optimized settings for faster response\n",
        "        llm = ChatGroq(\n",
        "            groq_api_key=groq_api_key,\n",
        "            model_name=\"llama-3.1-8b-instant\",\n",
        "            temperature=0.1,\n",
        "            max_tokens=150  # Reduced for shorter answers\n",
        "        )\n",
        "\n",
        "        # Extract PDF content (done once)\n",
        "        result = extract_pdf_from_url(pdf_url)\n",
        "        if not result:\n",
        "            return json.dumps({\"error\": \"Failed to extract PDF content\"})\n",
        "\n",
        "        # Create retriever (done once)\n",
        "        retriever = create_retriever_from_result(result)\n",
        "\n",
        "        # Parse queries\n",
        "        queries_data = json.loads(queries_json)\n",
        "        queries = queries_data.get(\"queries\", [])\n",
        "\n",
        "        answers = {}\n",
        "\n",
        "        # Optimized examples for concise responses\n",
        "        examples = \"\"\"Examples of concise policy answers:\n",
        "\n",
        "Q: What is the grace period for premium payment?\n",
        "A: 30 days grace period after due date.\n",
        "\n",
        "Q: What is the waiting period for pre-existing diseases?\n",
        "A: 36 months continuous coverage required.\n",
        "\n",
        "Q: Does this policy cover maternity expenses?\n",
        "A: Yes, after 24 months continuous coverage. Limited to 2 deliveries per policy period.\n",
        "\n",
        "Q: What is the waiting period for cataract surgery?\n",
        "A: 2 years waiting period.\"\"\"\n",
        "\n",
        "        # Process each query individually\n",
        "        for query in queries:\n",
        "            # Get relevant chunks for this specific query\n",
        "            relevant_chunks = retriever.retrieve(query, top_k=2)  # Reduced to 2 for faster processing\n",
        "\n",
        "            # Combine chunks for context\n",
        "            context = \"\\n\\n\".join([chunk for chunk, score in relevant_chunks])\n",
        "\n",
        "            # Create focused prompt for concise answers\n",
        "            prompt = f\"\"\"{examples}\n",
        "\n",
        "Based on the context below, provide a concise, specific answer:\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Q: {query}\n",
        "A:\"\"\"\n",
        "\n",
        "            # Get answer from ChatGroq\n",
        "            response = llm.invoke(prompt)\n",
        "            answer = response.content.strip()\n",
        "\n",
        "            # Clean the answer - remove any Q: and A: prefixes if present\n",
        "            if answer.startswith(\"Q:\"):\n",
        "                # Find the A: part and extract only the answer\n",
        "                a_index = answer.find(\"A:\")\n",
        "                if a_index != -1:\n",
        "                    answer = answer[a_index + 2:].strip()\n",
        "            elif answer.startswith(\"A:\"):\n",
        "                # Remove A: prefix\n",
        "                answer = answer[2:].strip()\n",
        "\n",
        "            # Store only the clean answer\n",
        "            answers[query] = answer\n",
        "\n",
        "        # Return compiled answers in clean JSON structure\n",
        "        return json.dumps({\"answers\": answers}, indent=2)\n",
        "\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": str(e)})\n",
        "\n",
        "\n",
        "# Usage example:\n",
        "pdf_url = \"https://hackrx.blob.core.windows.net/assets/policy.pdf?sv=2023-01-03&st=2025-07-04T09%3A11%3A24Z&se=2027-07-05T09%3A11%3A00Z&sr=b&sp=r&sig=N4a9OU0w0QXO6AOIBiu4bpl7AXvEZogeT%2FjUHNO7HzQ%3D\"\n",
        "\n",
        "queries_json = json.dumps({\n",
        "    \"queries\": [\n",
        "        \"will I get the cover if my wife died while delivery of the child\",\n",
        "        \"what is the premium amount\",\n",
        "        \"what are the exclusions in this policy\"\n",
        "    ]\n",
        "})\n",
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "# Execute\n",
        "result = process_pdf_queries(pdf_url, queries_json, groq_api_key)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QN1W8yTBEHI2",
        "outputId": "d1ea5b86-d065-4648-ce69-a21fcf99e745"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "success\n"
          ]
        }
      ],
      "source": [
        "# Fetch the secret\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "if groq_api_key:\n",
        "  print(\"success\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wf_YV41EHLC"
      },
      "outputs": [],
      "source": [
        "\n",
        "groq_api_key = userdata.get(\"GROQ_API_KEY\")\n",
        "\n",
        "\n",
        "# Execute the function\n",
        "result = process_pdf_queries(pdf_url, queries_json, groq_api_key)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qja_2Tb0EHOI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGMh8f_Ovfz4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUkEWKX7EHRp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
